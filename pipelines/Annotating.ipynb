{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating\n",
    "<img src=\"web_euplotid/lab_meeting_slides_Annotating.png\" style=\"width: 1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modified nucleosome localization data is used to color nodes\n",
    "After assembling the Insulated Neighborhood skeleton it is key to be able to visualize the impact of Histone modifications on the local regulatory structure, therefore we simply color the nodes of the genomic graph by histone modifications in the given cell state of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#overlap bed file w/ graph and return list of overlapping nodes\n",
    "def overlap_graph_bed(G, bed):\n",
    "    os.system(\"sort-bed \" + bed + \" | bedops -e -1 \" + \"all_nodes_sorted.bed - > \" + \"overlapped_nodes.bed\")\n",
    "    overlapped_nodes = set()\n",
    "    with open(\"overlapped_nodes.bed\",\"r\") as over_bed:\n",
    "        for node in over_bed:\n",
    "            arr = node.strip().split()\n",
    "            overlapped_nodes.add(str(arr[0]) + \":\" + str(arr[1]) + \"-\" + str(arr[2]))\n",
    "    os.remove(\"overlapped_nodes.bed\")\n",
    "    return overlapped_nodes\n",
    "\n",
    "#get classes of nodes using Histone ChIP-seq\n",
    "h3k4me3_nodes = overlap_graph_bed(dna_int_graph, prom_peaks)\n",
    "enh_nodes = overlap_graph_bed(dna_int_graph, enh_peaks)\n",
    "ctcf_nodes = overlap_graph_bed(dna_int_graph, ctcf_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Transcription Factor (TF) binding data is used to train neural networks capable of identifying TFs at chromatin accessibility sites\n",
    "Begin by training Convolutional Neural Networks (CNNs) based on all chip-seq and SELEX data for all TFs ever surveyed. The initial implementation of Euplotid uses pre-trained CNNs from [Deepbind](http://www.nature.com/nbt/journal/v33/n8/full/nbt.3300.html). These CNNs are able to identify the TFs which fall under each chromatin accessiblity peak, but in order to understand the peak as a whole Euplotid takes advantage of [Basset](http://genome.cshlp.org/content/early/2016/06/10/gr.200535.115.abstract) to train neural networks which are capable of predicting changes in chromatin accessibility. Basset is trained on all available chromatin accessibility data in ENCODE, DNAse of 180 different cell lines. Basset is therefore able to perform in-silico simulations to gauge the impact of a given mutation on the the complex as a whole, by combining this with the CNNs from Deepbind, we are able to make a prediction as to what factor is causing this change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Take all chromatin accessibility peaks within 50kb of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_openRegions_predict(G, homo_gen, chain_file, chain_file2, TF_RBP_ids,open_peaks, target_node_name, in_level, openpeak2bassetpeak):\n",
    "    #Function to add open chromatin accessibility peaks to graph and predict TF binding using DeepBind\n",
    "    #how far can the chromatin accessibility be from the node to be considered?\n",
    "    open_region_slop = 100000\n",
    "    orig_edges = G.edges()\n",
    "    with open(open_peaks,\"r\") as all_peaks:\n",
    "        for peak in all_peaks:\n",
    "            arr = peak.strip().split(\"|\")\n",
    "            if len(arr) > 1 and (arr[1] != \"NA\") and (abs(int(arr[2])) <= open_region_slop):\n",
    "                dist2anchor = abs(int(arr[2]))\n",
    "                arr_atac = arr[0].strip().split()\n",
    "                node_arr = arr[1].strip().split()\n",
    "                atac_node = arr_atac[0] + \":\" + str(int(arr_atac[1])) + \"-\" + str(int(arr_atac[2]))\n",
    "                open_size = float(arr_atac[3])\n",
    "                anchor_node = node_arr[0] + \":\" + str(int(node_arr[1])) + \"-\" + str(int(node_arr[2]))\n",
    "                if anchor_node in G.nodes():\n",
    "                    if in_level < 0:\n",
    "                        deep_open_pred_out = deepbind_predict_tf_range(arr_atac[0],int(arr_atac[1]),int(arr_atac[2]),homo_gen, TF_RBP_ids, target_node_name)\n",
    "                        #bass_open_pred_out = basset_predict_tf_range(arr_atac, homo_gen, target_node_name, openpeak2bassetpeak)\n",
    "                    else:\n",
    "                        deep_open_pred_out = [\"\",\"\",\"\"]\n",
    "                    top_tf_names = deep_open_pred_out[0]\n",
    "                    top_tf_probs = deep_open_pred_out[1]\n",
    "                    top_tf_probs = [str(round(prob,2)) for prob in top_tf_probs]\n",
    "                    top_tf_locs = deep_open_pred_out[2]\n",
    "                    predict_tf_deepbind = \"\"\n",
    "                    # Show top 15 TFs predicted under each chromatin accessibility peak\n",
    "                    for tf in range(0,min([len(top_tf_names),15])):\n",
    "                        predict_tf_deepbind += str(top_tf_names[tf]) + \" [\" + str(int(top_tf_locs[tf])) + \"] :\" + top_tf_probs[tf] + \", \"\n",
    "                    atac_mid = (int(arr_atac[1]) + int(arr_atac[2]))/2.0\n",
    "                    #add Differentially Methylated Cytosine (DMC) data from NGSMethDB if human\n",
    "                    if G.graph[\"species\"] == \"Human2\":\n",
    "                        DMC_node = get_NGSmethDMC(atac_node,chain_file,chain_file2)\n",
    "                    else:\n",
    "                        DMC_node = \"\"\n",
    "                    top_tf_names=(\"|\".join(top_tf_names))\n",
    "                    top_tf_probs=(\"|\".join(top_tf_probs))\n",
    "                    filt_tf_open = top_tf_names[0:min([len(top_tf_names),15])]\n",
    "                    G.add_node(atac_node, mid=atac_mid,name=atac_node, dist2anchor=dist2anchor,\n",
    "                              deepbind_tf=predict_tf_deepbind,top_tf_names=top_tf_names, filt_tf_open = filt_tf_open,\n",
    "                               top_tf_probs=top_tf_probs, DMC_node=DMC_node, open_size=open_size)\n",
    "                    G.add_edge(anchor_node,atac_node,label=\"\",weight=1.0)\n",
    "    os.system(\"bedmap --echo --echo-map-id-uniq --multidelim , --bp-ovr 1 \" + open_peaks + \" \" + encode_bed + \" > \" + target_node_name + \"_tfbs.bed\")\n",
    "    with open(target_node_name + \"_tfbs.bed\",\"r\") as open_tfbs_all:\n",
    "        for node_tf in open_tfbs_all:\n",
    "            arr = node_tf.strip().split(\"|\")\n",
    "            node_arr = arr[0].split()\n",
    "            node_name = node_arr[0] + \":\" + node_arr[1] + \"-\" + node_arr[2]\n",
    "            if len(arr)>1 and node_name in G:\n",
    "                G.node[node_name][\"tfbs\"] = arr[1].strip()\n",
    "    #cleanup\n",
    "    os.remove(target_node_name + \"_tfbs.bed\")\n",
    "    \n",
    "    #add predicted co-occurences\n",
    "    for edge in orig_edges:\n",
    "        left_tf = set()\n",
    "        right_tf = set()\n",
    "        left_tf_enc = set()\n",
    "        right_tf_enc = set()\n",
    "        #overlap top 15 TFs of each chromatin accessibility site as ordered by deepbind score\n",
    "        for neigh in G[edge[0]]:\n",
    "            if \"top_tf_names\" in G.node[neigh]:\n",
    "                left_tf |= set(G.node[neigh][\"top_tf_names\"].split(\"|\")[0:15])\n",
    "            if \"tfbs\" in G.node[neigh] and \"top_tf_names\" in G.node[neigh]:\n",
    "                left_tf_enc |= set(G.node[edge[0]][\"tfbs\"].strip().split(\",\"))\n",
    "        for neigh in G[edge[1]]:\n",
    "            if \"top_tf_names\" in G.node[neigh]:\n",
    "                left_tf |= set(G.node[neigh][\"top_tf_names\"].split(\"|\")[0:15])\n",
    "            if \"tfbs\" in G.node[neigh] and \"top_tf_names\" in G.node[neigh]:\n",
    "                right_tf_enc |= set(G.node[edge[0]][\"tfbs\"].strip().split(\",\"))\n",
    "        overlapped_tf = left_tf.intersection(right_tf)\n",
    "        overlapped_tf_enc = left_tf_enc.intersection(right_tf_enc)\n",
    "        if len(overlapped_tf) > 0:\n",
    "            over_names = \"\"\n",
    "            for tf in overlapped_tf:\n",
    "                over_names += tf + \", \"\n",
    "            G[edge[0]][edge[1]][\"overlapped_tf\"] = over_names\n",
    "        \n",
    "        if len(overlapped_tf_enc) > 0:\n",
    "            over_names_enc = \"\"\n",
    "            for tf in overlapped_tf_enc:\n",
    "                over_names_enc += tf + \", \"\n",
    "            G[edge[0]][edge[1]][\"overlapped_tf_enc\"] = over_names_enc\n",
    "        \n",
    "        if len(left_tf)>0:\n",
    "            G.node[edge[0]][\"filt_tf_peak\"] = \"|\".join(left_tf)\n",
    "        if len(right_tf)>0:\n",
    "            G.node[edge[1]][\"filt_tf_peak\"] = \"|\".join(right_tf)\n",
    "            \n",
    "        if len(left_tf_enc)>0:\n",
    "            G.node[edge[0]][\"tfbs_enc_all\"] = \"|\".join(left_tf_enc)\n",
    "        if len(right_tf_enc)>0:\n",
    "            G.node[edge[1]][\"tfbs_enc_all\"] = \"|\".join(right_tf_enc)\n",
    "            \n",
    "    #remove long prediction of annotations\n",
    "    for node in G.nodes():\n",
    "        if \"top_tf_names\" in G.node[node]:\n",
    "            G.node[node][\"top_tf_names\"] = \"\"\n",
    "            G.node[node][\"top_tf_probs\"] = \"\"\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Learned neural network can identify constituents of all chromatin accessibility peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deepbind_predict_tf_range(chrom, start, end, homo_gen, TF_RBP_ids, target_node_name):\n",
    "    size_window = 30\n",
    "    stagger_window = 5\n",
    "    deep_node_name = target_node_name.replace(\":\",\"_\").replace(\"-\",\"_\")\n",
    "    #filt TFs out w/ scores below this threshold\n",
    "    tf_filt = 2.0\n",
    "    open_seqs = list()\n",
    "    for left_bound in range(int(start),int(end),stagger_window):\n",
    "        open_seq = Seq.Seq(homo_gen.fetch(chrom,left_bound,left_bound+size_window))\n",
    "        ref_req = SeqRecord(open_seq, chrom+\":\"+str(left_bound)+\"-\"+str(left_bound+size_window),\"\",\"\")\n",
    "        open_seqs.append(ref_req)\n",
    "    open_seqs_fa = open(deep_node_name + \"_open_anchor.fa\", \"w+\")\n",
    "    #get deepbind id to TF name mapping\n",
    "    db_id2name = dict()\n",
    "    with open(TF_RBP_ids) as db_dp:\n",
    "        for db_id in db_dp:\n",
    "            db_id2name[db_id.split(\"#\")[0].strip()] = db_id.split(\"#\")[1].strip()\n",
    "    SeqIO.write(open_seqs, open_seqs_fa, \"fasta\")\n",
    "    open_seqs_fa.close()\n",
    "    #call deepbind and dump output\n",
    "    os.system(\"/root/deepbind/deepbind \" + TF_RBP_ids + \" \" +  deep_node_name + \"_open_anchor.fa\" + \" > \" + deep_node_name + \"_deep_open.txt\")\n",
    "    deep_open_out = open(deep_node_name + \"_deep_open.txt\",\"r+\")\n",
    "    header = deep_open_out.readline().strip().split()\n",
    "    deep_open_prob = np.loadtxt(deep_open_out,  ndmin = 2)\n",
    "    deep_open_out.close()\n",
    "    #smooth over overlapping windows and flatten array\n",
    "    deep_open_prob_smooth = list()\n",
    "    for tf in range(0,np.shape(deep_open_prob)[1]):\n",
    "        #deep_open_prob_smooth.extend(moving_average(deep_open_prob[:,tf],n=4))\n",
    "        deep_open_prob_smooth.extend(pd.rolling_max(deep_open_prob[:,tf],4)[3:len(deep_open_prob[:,tf])])\n",
    "    deep_probs_ix_sort = np.array(deep_open_prob_smooth).argsort()[::-1]\n",
    "    tf_names = [db_id2name[header[ix%np.shape(deep_open_prob)[1]]] \n",
    "                for ix in deep_probs_ix_sort if deep_open_prob_smooth[ix] >= tf_filt]\n",
    "    tf_probs = [deep_open_prob_smooth[ix] for ix in deep_probs_ix_sort \n",
    "                if deep_open_prob_smooth[ix] >= tf_filt]    \n",
    "    tf_loc =  [(np.floor(ix/np.shape(deep_open_prob)[1]) + int(start)) \n",
    "               for ix in deep_probs_ix_sort if deep_open_prob_smooth[ix] >= tf_filt]\n",
    "    os.remove(deep_node_name + \"_deep_open.txt\")\n",
    "    os.remove(deep_node_name + \"_open_anchor.fa\")\n",
    "    return [tf_names,tf_probs,tf_loc]\n",
    "\n",
    "def basset_predict_tf_range(open_region, homo_gen, target_node_name, openpeak2bassetpeak):\n",
    "    #trained DNAse peak model\n",
    "    model_file = \"/input_dir/pretrained_model.th\"\n",
    "    #Sequences stored in HDF5 format of encode DNAse peaks\n",
    "    seqs_file = \"/input_dir/encode_roadmap.h5\"\n",
    "    #table of DNAse target BED files used to train model\n",
    "    targets_file = \"/root/Basset/tutorials/sad_eg/sample_beds.txt\"\n",
    "    #expand region to at least 600 bp, default size model was trained on\n",
    "    homo_gen = pysam.FastaFile(\"/input_dir/hg19.fa\")\n",
    "    arr = re.split(r\"[-:]\",open_region)\n",
    "    chrom = arr[0]\n",
    "    start = arr[1]\n",
    "    end = arr[2]\n",
    "    stagger_window = 10\n",
    "    size_window = 600\n",
    "    open_seqs = list()\n",
    "    for left_bound in range(int(start),int(end),stagger_window):\n",
    "        open_seq = Seq.Seq(homo_gen.fetch(chrom,left_bound,left_bound+size_window))\n",
    "        ref_req = SeqRecord(open_seq, chrom+\":\"+str(left_bound)+\"-\"+str(left_bound+size_window),\"\",\"\")\n",
    "        open_seqs.append(ref_req)\n",
    "    with open(\"basset_open_anchor.fa\", \"w+\") as open_seqs_fa:\n",
    "        SeqIO.write(open_seqs, open_seqs_fa, \"fasta\")\n",
    "    cmd = (\"/root/Basset/src/seq_hdf5.py -r -c -v \" + str(len(open_seqs)) + \" -t \" + str(len(open_seqs)) \n",
    "           + \" basset_open_anchor.fa /input_dir/encode_roadmap_act.txt open_region.h5\")\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    cmd = (\"/root/Basset/src/basset_motifs.py -s \" + str(len(open_seqs)) + \" -t -o motifs_out + \" \n",
    "           + model_file + \" open_region.h5\")\n",
    "    subprocess.call(cmd, shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Take all SNPs/CNVs within footprint from dbSNP and annotate for:\n",
    "* eQTL in 32 tissues (FastQTL+GTeX)\n",
    "* Ease of CRISPR editing (GT-Scan2)\n",
    "* Results of over 2,000 GWAS studies (GRASP)\n",
    "* 667 Methylomes (NGSMethDB)\n",
    "* Estimate of deleteriousness (CADD Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_NGSmethDMC(open_region,chain_file,chain_file2):\n",
    "    open_region_hg38 =  liftover_chain(open_region,chain_file)\n",
    "    if open_region_hg38 is None:\n",
    "        return \"\"\n",
    "    ngs_methdb = \"http://bioinfo2.ugr.es:8888/NGSmethAPI/hg38/\" + open_region_hg38\n",
    "    meth_db_http = requests.get(ngs_methdb)\n",
    "    meth_db_out = \"\"\n",
    "    if meth_db_http.ok:\n",
    "        out_meth = meth_db_http.json()\n",
    "        if len(out_meth)>0:\n",
    "            for loc in out_meth:\n",
    "                if \"diffmeth_cg\" in loc:\n",
    "                    tissues_meth = set()\n",
    "                    pval_meth = list()\n",
    "                    for samp_a in loc[\"diffmeth_cg\"]:\n",
    "                        for samp_b in loc[\"diffmeth_cg\"][samp_a]:\n",
    "                            if \"methylKit\" in loc[\"diffmeth_cg\"][samp_a][samp_b] and (float(loc[\"diffmeth_cg\"][samp_a][samp_b][\"methylKit\"])<.01):\n",
    "                                pval_meth.append(float(loc[\"diffmeth_cg\"][samp_a][samp_b][\"methylKit\"]))\n",
    "                                tissues_meth |= set(samp_a.split(\"#\"))\n",
    "                                tissues_meth |= set(samp_b.split(\"#\"))\n",
    "                            if \"MOABS_sim\" in loc[\"diffmeth_cg\"][samp_a][samp_b] and (float(loc[\"diffmeth_cg\"][samp_a][samp_b][\"MOABS_sim\"])<.01):\n",
    "                                pval_meth.append(float(loc[\"diffmeth_cg\"][samp_a][samp_b][\"MOABS_sim\"]))\n",
    "                                tissues_meth |= set(samp_a.split(\"#\"))\n",
    "                                tissues_meth |= set(samp_b.split(\"#\"))\n",
    "                    if len(tissues_meth)>0:\n",
    "                        arr = re.split(r\"[-:]\",open_region)\n",
    "                        hg38_pos = arr[0] + \":\" + str(loc[\"pos\"]) + \"-\" + str(loc[\"pos\"]+1)\n",
    "                        hg19_pos = liftover_chain(hg38_pos,chain_file)\n",
    "                        meth_db_out += \"DMC:(\" + hg19_pos + \") number_tissues:\" + str(len(tissues_meth)) \\\n",
    "                        + \" min_pval:\" + str(round(min(pval_meth),9)) + \"\\n\"\n",
    "    return meth_db_out\n",
    "def add_variants_predict(G, homo_gen, chain_file, TF_RBP_ids, tissue_type, target_node_name,out_dir):\n",
    "    #Function to add variants and predict their effect using DeepBind\n",
    "    in_genes = G.node[target_node_name][\"in_name\"].split(\",\")\n",
    "    #get target gene for eQTL analysis\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    target_gene_out = mg.getgenes(in_genes, fields=\"ensembl\", species=\"human\", fetch_all=True)\n",
    "    ensembl_target = \"\"\n",
    "    for gene in target_gene_out:\n",
    "        if \"ensembl\" in gene:\n",
    "            ensembl_target = gene[\"ensembl\"][\"gene\"]\n",
    "    # flag for if any SNPs where added to the Graph\n",
    "    add_any_SNPs = False\n",
    "    #Save vcf output\n",
    "    basset_name_out = os.getcwd() + \"/\" + target_node_name.replace(\":\",\"_\").replace(\"-\",\"_\") + \"_basset\"\n",
    "    if not os.path.exists(basset_name_out):\n",
    "        os.makedirs(basset_name_out)\n",
    "    open_vcf = open(basset_name_out + \"/open_variants.vcf\",\"w+\")\n",
    "    #find all variants that fall within these open peaks using MyVariant.info \n",
    "    mv = myvariant.MyVariantInfo()\n",
    "    for node in G.nodes(data=True):\n",
    "        if \"dist2anchor\" in node[1]:\n",
    "            #Pull all variants within chromatin accessibility peak\n",
    "            all_snps = mv.query(node[0], fetch_all=True)\n",
    "            for snp in all_snps:\n",
    "                if \"dbsnp\" not in snp:\n",
    "                    continue\n",
    "                add_any_SNPs = True      \n",
    "                #Add CNVs/SNVs to graph\n",
    "                G.add_node(snp[\"_id\"],gen_start=snp[\"hg19\"][\"start\"], all_ref = str(snp[\"dbsnp\"][\"ref\"]),\n",
    "                           all_alt = str(snp[\"dbsnp\"][\"alt\"]),gen_end=snp[\"hg19\"][\"end\"], gen_chrom=\"chr\"+str(snp[\"chrom\"]), \n",
    "                           name=snp[\"_id\"])\n",
    "                G.add_edge(snp[\"_id\"], node[0],label=\"\",weight=1.0)\n",
    "                #print to VCF\n",
    "                open_vcf.write(\"chr\"+str(snp[\"chrom\"]) + \"\\t\" + str(snp[\"hg19\"][\"start\"]) + \"\\t\" + snp[\"_id\"] +\n",
    "                               \"\\t\" + str(snp[\"dbsnp\"][\"ref\"]) + \"\\t\" + str(snp[\"dbsnp\"][\"alt\"]) + \"\\n\")\n",
    "                \n",
    "                snp_loc = str(G.node[snp[\"_id\"]][\"gen_chrom\"]) + \":\" + str(G.node[snp[\"_id\"]][\"gen_start\"]) + \"-\" + str(G.node[snp[\"_id\"]][\"gen_end\"])\n",
    "                #add Differentially Methylated Cytosine (DMC) data from NGSMethDB\n",
    "                if G.graph[\"species\"] == \"Human2\":\n",
    "                    DMC_var = get_NGSmethDMC(snp_loc,chain_file,chain_file2)\n",
    "                    G.node[snp[\"_id\"]][\"DMC_node\"] = DMC_var\n",
    "                if (\"cadd\" in snp):\n",
    "                    G.node[snp[\"_id\"]][\"cadd\"] = snp[\"cadd\"][\"rawscore\"]\n",
    "                #add rsid if present and add GTEX eQTL pval and if target gene has Ensembl ID\n",
    "                if (ensembl_target is not None) and (\"dbsnp\" in snp) and (\"rsid\" in snp[\"dbsnp\"]):\n",
    "                    serv = \"http://rest.ensembl.org/eqtl/variant_name/homo_sapiens/\"\n",
    "                    ext = snp[\"dbsnp\"][\"rsid\"].strip() + \"?content-type=application/json;statistic=p-value;stable_id=\" + ensembl_target + \";tissue=\" + tissue_type \n",
    "                    r = requests.get(serv + ext, headers={ \"Content-Type\" : \"application/json\"})\n",
    "                    if r.ok:\n",
    "                        ret_val = r.json()\n",
    "                        if isinstance(ret_val,list) and (len(ret_val)>0):\n",
    "                            eqtl_pval = ret_val[0][\"value\"]\n",
    "                            G.node[snp[\"_id\"]][\"gtex_eqtl_pval\"] = round(eqtl_pval,6)\n",
    "                    G.node[snp[\"_id\"]][\"rsid\"] = snp[\"dbsnp\"][\"rsid\"]\n",
    "                #add GRASP phenotype and PMID information for SNP\n",
    "                if (\"grasp\" in snp) and (\"publication\" in snp[\"grasp\"]):\n",
    "                    pheno_gwas = \"\"\n",
    "                    pmid_gwas = \"\"\n",
    "                    #single or multiple publications w/ SNP?\n",
    "                    if isinstance(snp[\"grasp\"][\"publication\"], list):\n",
    "                        for pub in snp[\"grasp\"][\"publication\"]:\n",
    "                            if isinstance(pub[\"paper_phenotype_description\"],list):\n",
    "                                pheno_gwas += \",\".join([str(pheno) + \", \" for pheno in pub[\"paper_phenotype_description\"]]) + \", \"\n",
    "                            else:\n",
    "                                pheno_gwas += pub[\"paper_phenotype_description\"] + \",\"\n",
    "                            if isinstance(pub[\"pmid\"],list):\n",
    "                                pmid_gwas += \",\".join([str(pmid) for pmid in pub[\"pmid\"]]) + \", \"\n",
    "                            else:\n",
    "                                pmid_gwas += str(pub[\"pmid\"]) + \",\"\n",
    "                    else:\n",
    "                        pub = snp[\"grasp\"][\"publication\"]\n",
    "                        if isinstance(pub[\"paper_phenotype_description\"],list):\n",
    "                            pheno_gwas += \"\".join([str(pheno) + \", \" for pheno in pub[\"paper_phenotype_description\"]]) + \", \"\n",
    "                        else:\n",
    "                            pheno_gwas += pub[\"paper_phenotype_description\"]\n",
    "                        if isinstance(pub[\"pmid\"],list):\n",
    "                            pmid_gwas += \",\".join([str(pmid) for pmid in pub[\"pmid\"]])\n",
    "                        else:\n",
    "                            pmid_gwas += str(pub[\"pmid\"])\n",
    "                    G.node[snp[\"_id\"]][\"grasp_pheno\"] = pheno_gwas.replace(\" \",\"_\")\n",
    "                    G.node[snp[\"_id\"]][\"grasp_pmid\"] = pmid_gwas.replace(\" \",\"_\")\n",
    "    open_vcf.close()\n",
    "    #predict effect of all variation added to graph using DeepBind and Basset\n",
    "    if add_any_SNPs:\n",
    "        print(\"Deepbind SNP prediction of: \" + target_node_name)\n",
    "        G = deepbind_predict_SNPs(G, homo_gen, TF_RBP_ids,target_node_name)\n",
    "        print(\"Basset SNP prediction of: \" + target_node_name)\n",
    "        G = add_basset_sad_sat(G, homo_gen, target_node_name,out_dir)\n",
    "    try:\n",
    "        os.remove(basset_name_out + \"open_variants.vcf\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    shutil.rmtree(basset_name_out)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Neural network is able to predict the effect of mutations within CREs by performing in-silico mutational screen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deepbind_predict_SNPs(G, homo_gen, TF_RBP_ids,target_node_name):                        \n",
    "    #predict effect of CNVs/SNPs which were added to Insulated Neighborhood\n",
    "    flank_snp = 15\n",
    "    mut_seqs = list()\n",
    "    ref_seqs = list()\n",
    "    snp_names = list()\n",
    "    mut_snps = open(target_node_name + \"_mut_snps.fa\", \"w+\")\n",
    "    ref_snps = open(target_node_name + \"_ref_snps.fa\", \"w+\")\n",
    "    #get deepbind id to TF name mapping\n",
    "    db_id2name = dict()\n",
    "    with open(TF_RBP_ids) as db_dp:\n",
    "        for db_id in db_dp:\n",
    "            db_id2name[db_id.split(\"#\")[0].strip()] = db_id.split(\"#\")[1].strip()\n",
    "    for node in G.nodes(data=True):\n",
    "        if \"all_ref\" in node[1]:\n",
    "            chrom = node[1][\"gen_chrom\"]\n",
    "            snp_loc = node[1][\"gen_start\"]\n",
    "            snp_names.append(node[0])\n",
    "            #get 5 tiled sequences and average the probability across them\n",
    "            for offset in range(-2,3):\n",
    "                seq_start = int((snp_loc-flank_snp)+(offset*5.0))\n",
    "                seq_end = int((snp_loc+flank_snp+1)+(offset*5.0))\n",
    "                snp_seq = Seq.Seq(homo_gen.fetch(chrom,seq_start,seq_end))\n",
    "                ref_seq = snp_seq.tomutable()\n",
    "                ref_req = SeqRecord(ref_seq, node[0],\"\",\"\")\n",
    "                ref_seqs.append(ref_req)\n",
    "                mut_seq = snp_seq.tomutable()\n",
    "                alt_seq = node[1][\"all_alt\"]\n",
    "                if len(alt_seq) > 1:\n",
    "                    max_ix = min(len(mut_seq),(flank_snp-1+len(alt_seq)))\n",
    "                    mut_seq[flank_snp-1:max_ix] = alt_seq\n",
    "                else:\n",
    "                    mut_seq[flank_snp-1] = node[1][\"all_alt\"]\n",
    "                mut_req = SeqRecord(mut_seq, node[0],\"\",\"\")\n",
    "                mut_seqs.append(mut_req)\n",
    "    #write sorrounding sequences of SNP to file and reopen again for deepbind\n",
    "    SeqIO.write(mut_seqs, mut_snps, \"fasta\")\n",
    "    SeqIO.write(ref_seqs, ref_snps, \"fasta\")\n",
    "    mut_snps.close()\n",
    "    ref_snps.close()\n",
    "    #call deepbind and dump output\n",
    "    os.system(\"/root/deepbind/deepbind \" + TF_RBP_ids + \" \" +  target_node_name + \"_mut_snps.fa\" + \" > \" + target_node_name + \"_deep_mut.txt\")\n",
    "    os.system(\"/root/deepbind/deepbind \" + TF_RBP_ids + \" \" +  target_node_name + \"_ref_snps.fa\" + \" > \" + target_node_name + \"_deep_ref.txt\")\n",
    "    #open deepbind output for ref and mut\n",
    "    deep_mut_out = open(target_node_name + \"_deep_mut.txt\",\"r+\")\n",
    "    deep_ref_out = open(target_node_name + \"_deep_ref.txt\",\"r+\")\n",
    "    #peel header off\n",
    "    header = deep_mut_out.readline().strip().split() \n",
    "    header = deep_ref_out.readline().strip().split() \n",
    "    #suck deebind output into numpy array\n",
    "    deep_mut_prob = np.loadtxt(deep_mut_out,  ndmin = 2)\n",
    "    deep_ref_prob = np.loadtxt(deep_ref_out,  ndmin = 2)\n",
    "    deep_mut_out.close()\n",
    "    deep_ref_out.close()\n",
    "    for snp_num in range(0,len(snp_names)):\n",
    "        #compare scores of TF binding according to Deepbind paper\n",
    "        deep_ref_prob_cut = np.mean(deep_ref_prob[(snp_num*5):((snp_num+1)*5),:],axis=0)\n",
    "        deep_mut_prob_cut = np.mean(deep_mut_prob[(snp_num*5):((snp_num+1)*5),:],axis=0)\n",
    "        max_diffs = np.maximum.reduce([np.zeros(len(deep_ref_prob_cut)),deep_ref_prob_cut,deep_mut_prob_cut])      \n",
    "        deep_cut_prob = (deep_ref_prob_cut - deep_mut_prob_cut) * max_diffs\n",
    "        max_shown = min(5, sum(abs(deep_cut_prob)>=3))\n",
    "        if max_shown > 0:\n",
    "            temp_sort = np.array(abs(deep_cut_prob)).argsort()[::-1]\n",
    "            deep_probs_ix_sort = [ix for ix in temp_sort if np.isfinite(deep_cut_prob[ix])][0:max_shown]\n",
    "            out_prob_name = \"\"\n",
    "            top_tf_names = [db_id2name[header[ix]] for ix in deep_probs_ix_sort]\n",
    "            top_tf_probs = [deep_cut_prob[ix] for ix in deep_probs_ix_sort]\n",
    "            for x in zip(top_tf_names,top_tf_probs):\n",
    "                out_prob_name += str(x[0]) + \":\" + str(round(x[1],2)) + \", \"\n",
    "            G.node[snp_names[snp_num]][\"deep_score\"] = out_prob_name\n",
    "    os.remove(target_node_name + \"_deep_mut.txt\")\n",
    "    os.remove(target_node_name + \"_deep_ref.txt\")\n",
    "    os.remove(target_node_name + \"_mut_snps.fa\")\n",
    "    os.remove(target_node_name + \"_ref_snps.fa\")\n",
    "    return G\n",
    "\n",
    "def add_basset_sad_sat(G, homo_gen, target_node_name,out_dir):\n",
    "    basset_out_name = out_dir + \"/\" + target_node_name.replace(\":\",\"_\").replace(\"-\",\"_\") + \"_basset\"\n",
    "    target_in_name = dna_int_graph.node[target_node_name][\"in_name\"]\n",
    "    #trained DNAse peak model\n",
    "    model_file = \"/input_dir/pretrained_model.th\"\n",
    "    #Sequences stored in HDF5 format of encode DNAse peaks\n",
    "    seqs_file = \"/input_dir/encode_roadmap.h5\"\n",
    "    #table of DNAse target BED files used to train model\n",
    "    targets_file = \"/root/Basset/tutorials/sad_eg/sample_beds.txt\"\n",
    "    targetID_celltype = dict()\n",
    "    count=0\n",
    "    with open(targets_file) as id2type:\n",
    "        for c_type in id2type:\n",
    "            arr = c_type.strip().split()\n",
    "            targetID_celltype[arr[0]] = count\n",
    "            count += 1\n",
    "    cmd = (\"/root/Basset/src/basset_sad.py -f \" + (homo_gen.filename).decode(\"utf-8\") + \" -l 600 -o \" + basset_out_name + \" -t \" + targets_file + \" \" +  model_file + \" \" + basset_out_name + \"/open_variants.vcf\")\n",
    "    print(cmd)\n",
    "    os.system(cmd)\n",
    "    #Pick SNP maximizing for sum of Delta SAD across all cell types in trained model\n",
    "    sad_table = pd.read_table(basset_out_name + \"/sad_table.txt\", delim_whitespace=True, header = 0)\n",
    "    sad_table_dense = sad_table.pivot_table(index='rsid', columns='target', values='pred')\n",
    "    serial_sad_table = pickle.dumps(sad_table_dense, protocol=0)\n",
    "    G.graph[\"sad_table\"] = serial_sad_table\n",
    "    sad_table[\"sad_abs\"] = abs(sad_table[\"pred\"])\n",
    "    sad_rsid_table = sad_table.groupby(\"rsid\").sum()\n",
    "    sad_rsid_table = sad_rsid_table.sort_values(by=\"sad_abs\",ascending=False)\n",
    "    #Store SAD in each SNP node\n",
    "    top_snp_names = sad_rsid_table.index\n",
    "    for snp_index, snp_row in sad_rsid_table.iterrows():\n",
    "        G.node[snp_index][\"sad_abs_sum\"] = snp_row[\"sad_abs\"]\n",
    "        G.node[snp_index][\"sad_pred\"] = \"\"\n",
    "    for sad_index, sad_row in sad_table.iterrows():\n",
    "        G.node[sad_row[\"rsid\"]][\"sad_pred\"] += str(round(sad_row[\"pred\"],4)) + \",\"\n",
    "    target_sad_name = (top_snp_names[0].replace(\":\",\"_\")).replace(\">\",\"_\")\n",
    "    top_snp_sad_abs_sum = sad_rsid_table[\"sad_abs\"] \n",
    "    top_snp_sad_profile = sad_table[sad_table[\"rsid\"].str.contains(top_snp_names[0])]\n",
    "    top_snp_ix = top_snp_sad_profile[\"sad_abs\"].argmax()\n",
    "    top_snp_sad_table = top_snp_sad_profile.ix[top_snp_ix] \n",
    "    top_snp_ctype_target = targetID_celltype[top_snp_sad_table[\"target\"]]\n",
    "    top_snp_node = G.node[top_snp_names[0]]\n",
    "    with open(basset_out_name + \"/top_variant.vcf\",\"w\") as filt_out:\n",
    "        filt_out.write(str(top_snp_node[\"gen_chrom\"]) + \"\\t\" + str(top_snp_node[\"gen_start\"]) + \"\\t\" + top_snp_names[0] + \n",
    "                        \"\\t\" + top_snp_node[\"all_ref\"] + \"\\t\" + top_snp_node[\"all_alt\"] + \"\\n\")\n",
    "    #flag top SNP in open peak\n",
    "    G.node[top_snp_names[0]][\"top_open_snp\"] = True\n",
    "    #Perform in-silico mutagenesis on top SNP to generate output PDFs\n",
    "    cmd = (\"/root/Basset/src/basset_sat_vcf.py -f \" + (homo_gen.filename).decode(\"utf-8\") + \" -t \" + str(top_snp_ctype_target) + \" -o \" + basset_out_name + \n",
    "           \" \" + model_file + \" \" + basset_out_name + \"/top_variant.vcf\")\n",
    "    os.system(cmd)\n",
    "    #Save output png & cleanup\n",
    "    #tag in JSON file too\n",
    "    G.node[top_snp_names[0]][\"sad_pdf\"] = target_in_name + \"_sad_heat.png\"\n",
    "    out_pdfs = [pdf for pdf in glob.glob(basset_out_name+\"/*heat.png\") if target_sad_name in pdf]\n",
    "    label_snp = [\"ref\",\"alt\"]\n",
    "    count = 1\n",
    "    for pdf in out_pdfs:\n",
    "        pdf_sat = pdf.split(\"/\")\n",
    "        G.node[top_snp_names[0]][\"sad_mut_\"+label_snp[count%2]] = str(pdf_sat[len(pdf_sat)-1])\n",
    "        count += 1\n",
    "        if os.path.isfile(pdf):\n",
    "            os.rename(pdf, out_dir+\"/\"+pdf_sat[len(pdf_sat)-1])\n",
    "    return G"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
